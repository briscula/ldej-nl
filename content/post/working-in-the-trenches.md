---
title: "Working in the Trenches"
date: 2020-07-09T17:58:21+05:30
draft: false
---

Whenever I join a project I'm curious to find out what the situation is like. Which technology stack are they using? What is the developer experience like? What is the technical debt? Are they doing agile development? I like to be surprised in a positive way, but for this project it was rather the opposite. 

I'm not going to disclose too many details as I don't want to call them out publicly. I will say that it's a project in the educational industry.

One of the first things I notice is that their Gitlab contains 1000+ repositories. I'm working on one product but there are multiple products in these repositories. This is either going to be a well structured project or a splintered mess. The project is just a couple of years old and has less than 100 employees and a number of external parties developing for them.

According to themselves they are using a microservices architecture. Some services are Java Spring and the newer ones are NodeJS. There is a main NodeJS service which contains the main application. This application does not communicate to any of the (micro)services directly. Instead it is the Javascript frontend code that communicates to a gateway which will route the api calls to the right service. This means that if your functionality requires calls from multiple services, the frontend needs to sequentially do calls to those services. The scoping of what belongs is which service isn't always clear. The main application is supposed to contain the core functionality, the services should stay within their specific bounded contexts.

It turns out that their role based access control (rbac) is a separate service. This has the downside that whenever an object is stored in the database via the main service, a separate call needs to be done to the rbac service to register who has access to that object. The apis for the rbac service do not contain much functionality and are merely endpoints for receiving and storing objects. So in the case of the rbac service this means we need to do two calls. One to retrieve the object we want to modify and one to store the modified object. As a result, some functionality does three or more sequential calls to services from the browser. In some cases you are looking at a spinner on a greyed out web page for a number of seconds.

I joined the project to look at some of the performance problems. There were a couple of screens related to rbac functionality that resulted in "kill page or wait" when loaded. After some inspection it turns out that the rbac service is returning a 2.6MB compressed json response. It contains many tens of thousands of objects which were all rendered by AngularJS resulting in millions of html nodes. After three minutes of loading and the browser tab taking up over 1.5GB of memory the page loads. This happened on an environment which they were using internally, but it is still unclear to me how they could have worked like this for a number of months. The quick fix was easy: many of these tens of thousands of objects were duplicates which should not have existed. After consulting with them they clean up the data and the page loads in a manageable couple of seconds.

The underlying problem is a bit bigger though. The objects returned by the API are exact documents coming from MongoDB and contain much more information than required by the interface. Projection would cut down the response size by at least half. There is no pagination for the API call and there is no pagination in the interface which resulted in all these objects being rendered.

While working on this issue I find out that there is no documentation for the main service and no documentation for the rbac service. There is no API documentation either, not a single test, no linting or formatting and no developer who could tell me how things were supposed to work. This is a pattern we will see repeating throughout this project.

The project has around 40 services total. Searching through the main project code I find API calls to maybe a handful of services. These services are the generic services like the identity service, chat service and the rbac service. Where are all the other services used?

In this system users can create pages. They can contain text and images but they can also include what they call applets. An applet has specific functionality and can communicate with the backend services. Some services are a way to communicate with external parties, others contain specific functionality. These services can store data in MongoDB. The MongoDB instance contains multiple databases. I expected a separate database for each backend service but instead multiple services connect to the same databases and access and store the same data. This means that some of these services communicate via the database. No code is shared between these services so in case the structure of an object is changed you need to search through all these repositories and adjust the implementation in all of them. The functionality and APIs for these services are undocumented and do not contain a single test. The APIs are not versioned.

Back to the applets. The code for these applets does not reside in the main application nor does it live in any of the backend services and it cannot be found in any of the 1000+ code repositories. Instead this code is coming from the database. Each environment (local, dev, stage, prod) has it's own database and can therefore contain different applets and different versions of applets.

The applets can be edited via the administrator section of the interface. The interface has two editor fields which, on my screen, are about 30 characters wide and 15 lines high. One called "Processor" which is where most of the Javascript of the applet lives, and "Data Template" which contains the html, css and in some cases some inlined Javascript. For a decently sized applet there are around one to two thousand lines in each editor. The editor does have syntax highlighting, but that is where the developer experience ends. There are no tests or possibility for tests, there is no documentation, there is no explicit versioning, no change log, no version control. It is unclear which Javascript libraries are available let alone which version of them. There is no preview of the applet. There is no check to see if your code contains syntax errors, there is no validation against backend services. It is unclear which backend services the applet uses or which backend services and APIs are available.

In case a backend service is used by this applet it usually contains some commented out code. This code specifies to which hostname the applet should connect to communicate with the backend service. By default it will connect to the gateway configured in the main application which will dispatch the call to the backend service. When you make a checkout of the code, that default is the dev environment (more on dev that later). This means that your applets will connect the backend services of the default environment. If you want to connect it to the backend service running locally, because you need change something there, you can comment out the code in the applet that will make it connect to localhost.

Unfortunately we need to go one level deeper. This is a multi-tenant platform. In this case the tenants are universities and colleges that can reside next to each other on the same environment. Each of these tenants can have a different version of an applet.

Managing different versions of applets for multiple tenants in multiple environments is not easy task. Especially if you don't know which backend services these applets require and especially when the code is not version controlled. To overcome this problem there is an applet store. A separate project which contains each version of each applet and which can make these versions available to specific tenants on specific environments. The applet store is only accessible by specific employees. Tenants can access this applet store through their interface and enable applets for their environment. They also have the possibility to add applets themselves. However, I have no idea how anyone would understand how to do this without documentation. Tenants also have the ability to customise the applets available from the store. When they do that the applet available through the applet store will be copied and they can make adjustments. When that happens, the customised applet will not be updated when the version in the applet store will be updated, which means they might miss out on bug fixes.

In some cases tenants request a specific applet to be made for them and where it doesn't make sense to make this applet available to all tenants. Maybe it integrates with their custom system. In this case you have to options: either you make the applet tenant specific so it will only show up for that tenant, or you add the code directly the same way a tenant would add a custom applet.

Let's say you get a bug report of a tenant stating that a certain applet has a defect. How do you find out where the problem is? Is there a bug in the applet available in the store? Has the tenant updated to the latest version of the applet? Did the tenant customise the applet? Is this a tenant specific applet? Are the right backend services available for the applet to work? Or are they the wrong version? Does this applet require a specific version of a Javascript library which has not been deployed yet?

Without documentation, version control, dependency management and tests it can take days to figure out where the code is coming from that is causing the defect. In the meantime the tenant may update the applet, an employee may update the applet or a deployment might happen.

Changing anything in any applet or backend service feels like juggling with loaded guns.

As a developer you want to be able to experiment without breaking things for other people. Using a central environment or database for developers is unfortunately a common practise. This project was suffering the same faith. Remember the dev environment I referred to? The dev environment is what developers work against. You run the main project locally, but everything else is coming from dev. The applets will connect to the gateway of dev by default. This is how many developers work but it also creates a single point of failure. If any developer experiments a bit too much, all developers might be stuck due to a broken environment. Next to that, who is going to make sure that the database is in any way representative of the other environments?

There is an option for developers to get their database to run locally. But this also means you run out-of-sync quickly with the other environments. Also, the applets will still connect to backend services on dev, using the dev database. This leads to data incompatibilities. Running all 40 services locally is not an option. After starting the 8th service my machine completely froze, incapable of handling all those memory hungry jvms.

The dev environment requires 7 machines. These machines were not hosted in any of the big cloud providers, instead they were managed by a cloud provider. There is no web interface to see or manage anything when it comes to instances. You basically have to live with what is provided. The only way to get another instance is by requesting it (with a very good reason), getting it approved and waiting a number of days. The dev machines are running an older version of an operation system and have not seen a security patch since the day they were created.

Let's say you made changes to code and feel confident that it is going to work. When your changes are merged you need to get them deployed to the dev environment. The way to do that is by going to their Jenkins instance which, as you may expect by now, has not been updated since the day it has been setup. Jenkins contains many jobs for building services, in fact it also contains many views (groups of jobs). Nobody had ever invested time in setting up a good build pipeline. They had never heard of or seen the option to parameterise jobs. As a result they copied each job for each service for every release for every environment. Over a small period of time this has lead to a completely filled up hard disk and the regular task of cleaning up jobs in Jenkins to make space.

We are not there yet, you want your main application to be deployed right? Unfortunately the main application is a bit more difficult when it comes to building, so it is not a job in Jenkins. Instead you need to ssh into the Jenkins machine itself. You need to checkout the right branch for your repository and run the right script for building the docker containers and pushing them. Oh and make sure to use the right build script as there is a different one for each environment. The only difference between these scripts is (or actually was supposed to be) the docker image tag. Please be careful with these scripts as they are not part of any code repository so we cannot track who changes what. Also, the docker images, which include the code and keys to external parties, are pushed to the public docker hub. 

Alright, you got the images for your main application build. Now to get them deployed, ssh into the first machine of the dev environment, checkout the right branch of the repository with deployment scripts and run the one for dev.



The developer experience was terrible. You never know where to find code, there as no documentation, no tests, you are never confident that your change won't break anything and you had to work with unreliable environments. Your work grinds to a halt often due to a broken dev environment. Testing, building and deploying were manual steps that would often fail in mysterious ways.

The dev environment, and all other environments for that matter, did not have any aggregated logging, monitoring or alerting. The logs for dev could be accessed by checking the docker logs of the containers. We did not have access to other environments. Logs were not traceable for requests or for users. Quite often services would be down for extended periods of time without anybody noticing. Issues on production would often be reported by tenants or their users. Issues would be debugged by trying to reproduce them on dev or locally, many times without avail.

I started this story with a number of questions. One of them is: "Are they doing agile development?" Well, the short answer is no, but they wanted to. They work in sprints of 4 weeks. That's a bit long to my taste. However, that is not the worst part of this. Each sprint consists of 2 weeks of development, 1 week of testing on QA and 1 week of testing on staging after which the deployment to production would happen. You might be wondering what the developers are doing during the other two weeks. They are fixing bugs that have been introduced by their changes that have been tested, they are fixing bugs that have been reported by their customers and quite often they continue developing on the items that did not manage to be finished in the 2 weeks of development.

During the last 2 weeks of the sprint there are what they call grooming sessions of about an hour every day. In a grooming session the various product owners walk through the stories that are lined up for the coming sprint. The stories include requests from tenants, internal requests for more functionality and some reported bugs. Stories vary in size from a small enhancement to work that might take more than a sprint. Requests from tenants can be bugs or enhancements, quite often they were enhancements disguised as bugs. In case they are actual bugs they are often not more than a small email, possibly with a screenshot. In some cases the environment and login credentials would be mentioned, but these credentials would not be tested and the bug would not be reproduced or confirmed before the grooming session. 

The grooming sessions are attended by product owners, developers, testers and the architect. The sessions consist mostly of walking through the newly requested functionality and most of the questions are focused on the functionality itself. The technical aspects of stories would often not be discussed at all or would be deferred to other meetings.

Planning of what would fit in the sprint is difficult, even if you are very good at splitting up stories and making estimations. However, in this case the estimation is done by the product owners before the grooming sessions. They would grade the complexity of a story as "easy", "medium" or "hard". How do you know if the groomed stories fit in the sprint? You don't. Nobody looks at the capacity, nobody tracks the velocity. In the end this means that every sprint will be filled up with items that the product owners would like to see in that sprint. This also results in every sprint not being finished on time and big stories being completely transferred to the next sprint.

If you look at the types of stories I mentioned you might see that a critical one is missing: technical debt. Who is going to automate deployment? Who makes time for setting up a testing framework? Who is going to upgrade dependencies? No time is planned for any of the activities that make sure that a developer is working in a workable environment. Because of this, dependencies of all services were stuck on where they were when they got included, the MongoDB database was stuck on it's initial version, the operating systems were living in the past. If you don't plan time for keeping up with the essentials, then how do you make time for improving the situation?
