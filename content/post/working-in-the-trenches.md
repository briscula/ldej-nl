---
title: "Working in the Trenches"
date: 2020-07-09T17:58:21+05:30
draft: false
summary: "I joined a project which turns out to have substantial technical debt."

reddit:
  created: 1594708583 
  url: https://www.reddit.com/r/ldej/comments/hqwgj8/discuss_working_in_the_trenches/
  title: "Discuss: Working in the Trenches"
---

Whenever I join a project I'm curious to find out what the situation is like. Which technology stack are they using? What is the developer experience like? What is the technical debt? Are they doing agile development? I like to be positively surprised, but for this project it was rather the contrary.

One of the first things I notice is that their Gitlab contains 1000+ repositories. I'm working on one product but there are multiple products in these repositories. This is either going to be a well-structured project, or a splintered mess. The project is just a couple of years old and has less than 100 employees, and a number of external parties are developing for them.

According to themselves they are using a microservices architecture. Some services are Java Spring and the newer ones are NodeJS. There is a main NodeJS service which contains the main application. This application does not communicate to any of the (micro)services directly. Instead, it is the Javascript frontend code that communicates to a gateway which will route the api calls to the right service. This means that if your functionality requires calls from multiple services, the frontend needs to sequentially do calls to those services. The scoping of what belongs in which service isn't clear. The main application is supposed to contain the core functionality, the services should stay within their specific bounded contexts.

It turns out that their role based access control (rbac) is a separate service. This has the downside that whenever an object is stored in the database via the main application, a separate call needs to be done to the rbac service to register who has access to that object. The apis for the rbac service do not contain much functionality and are merely endpoints for retrieving from and storing in their MongoDB document store. So in the case of the rbac service this means we need to do two calls. One to retrieve the document we want to modify and one to store the modified object. As a result, some functionality requires three or more sequential calls to services from the browser. In some cases you are looking at a spinner on a greyed out web page for a number of seconds.

My first task is to look at some performance problems. There are a couple of screens related to rbac functionality that result in "kill page or wait" when loaded. After some inspection it turns out that the rbac service is returning a 2.6MB compressed json response. It contains many tens of thousands of document which are all rendered by AngularJS resulting in millions of html nodes. After three minutes of loading, and the browser tab taking up over 1.5GB of memory, the page loads. This happened on an environment which they were using internally, but it is still unclear to me how they could have worked like this for a number of months. The quick fix is easy: many of these tens of thousands of objects are duplicates which should not have existed. After consulting with them they clean up the data, and the page loads in a manageable couple of seconds.

The underlying problem is a bit bigger though. The json returned by the API are exact documents coming from MongoDB and contain much more information than required by the interface. Projection would cut down the response size by at least half. There is no pagination for the API call and there is no pagination in the interface which resulted in all these objects being rendered.

While working on this issue I find out that there is no documentation for the main application and no documentation for the rbac service. There is no API documentation either, not a single test and no linting or formatting. This is a pattern you will see repeating throughout this project.

The project has around 40 services total. Searching through the main project code I find API calls to maybe a handful of services. These services are generic services like the identity service, chat service, and the rbac service. Where are all the other services used?

In this system users can create pages. They can contain text and images, but they can also include what they call applets. Applets are pieces of Javascript, html and css. An applet has specific functionality and can communicate with the backend services. Some services communicate with external parties, others interact with MongoDB. The MongoDB instance contains multiple databases. I expected a separate database for each backend service but instead multiple services connect to the same databases and access and store the same data. This means that they communicate via the database. No code is shared between these services so in case the structure of a document is changed you need to search through all these repositories and adjust the implementation in all of them. The functionality and APIs for these services are undocumented and do not contain a single test. The APIs are not versioned.

Back to the applets. The code for these applets does not reside in the main application nor does it live in any of the backend services, and it cannot be found in any of the 1000+ repositories. Instead, this code is coming from the database. Each environment (local, dev, stage, prod) has its own database and can therefore contain different applets and different versions of applets.

The applets can be edited via the admin functionality of the main application. The interface has two editor fields which, on my screen, are about 30 characters wide and 15 lines high. Just enough to not see anything. The one called "Processor" is where most of the Javascript of the applet lives, and once called "Data Template" that contains the html, css and in some cases inlined Javascript. For a decently sized applet there are around one to two thousand lines in each editor. The editor does have syntax highlighting, but that is where the developer experience ends. There are no tests or possibility for tests, there is no documentation, there is no explicit versioning, no change log, no version control. It is unclear which Javascript libraries are available let alone which version of them. There is no preview of the applet. There is no check to see if your code contains syntax errors, there is no validation against backend services. It is unclear which backend services the applet uses or which backend services and APIs are available at all.

In case a backend service is used by this applet it usually contains commented out code. This code specifies to which host the applet should connect to communicate with the backend service. By default, it will connect to the gateway configured in the main application which will dispatch the call to the backend service. When you make a checkout of the code, that default is the dev environment (more on dev that later). This means that your applets will connect the backend services of the dev environment. If you are working on a backend service and are running it locally, you need to comment out the code to make the applet connect to localhost.

Unfortunately we need to go one level deeper. This is a multi-tenant platform. In this case the tenants are universities and colleges that can reside next to each other in the same environment. Each of these tenants can have different versions of an applet.

Managing different versions of applets for multiple tenants in multiple environments is not easy task. Especially if you don't know which backend services these applets require and especially when the code is not version controlled. To overcome this problem there is an applet store. A separate project which contains each version of each applet and which can make these versions available to specific tenants on specific environments. The applet store application is only accessible by employees. Tenants can access this applet store through the interface of the main application and enable applets for themselves. They also have the possibility to create new applets by themselves. However, I have no idea how anyone would understand how to do this without documentation. Tenants also have the ability to customise the applets available from the store. When they do that the applet available through the applet store will be copied, and then they can make adjustments. When an applet is copied, the customised applet will not be updated when the version in the applet store will be updated, which means they might miss out on bug fixes.

Tenants also request custom applets to be made for them, for example to integrate with a custom system. In this case you have to options: either you make the applet tenant specific, so it will only show up for that tenant, or you add the code directly the same way a tenant can create a custom applet.

Let's say you get a bug report of a tenant saying that a certain applet has a defect. How do you find out where the problem is? Is there a bug in the applet available in the store? Has the tenant updated to the latest version of the applet? Did the tenant customise the applet? Is this a tenant specific applet? Are the right backend services available for the applet to work? Is the backend service the right version? Does this applet require a specific version of a Javascript library which has not been deployed yet?

Without documentation, version control, dependency management and tests it can take days to figure out where the code is coming from that is causing the defect. In the meantime the tenant may update the applet, an employee may update the applet, or a deployment might happen.

Changing anything in any applet or backend service feels like juggling with loaded guns.

## Developer experience

As a developer you want to be able to experiment without breaking things for other people. Using a central environment or database for developers is unfortunately a common practise. This project is suffering the same faith. Remember the dev environment I referred to? The dev environment is what developers work against. You run the main application locally, but everything else is coming from dev. The applets will connect to the gateway of dev by default. This is how many developers work, but it also creates a single point of failure. If any developer experiments a bit too much, all developers can be stuck due to a broken environment. Next to that, who is going to make sure the database is in any way representative of the other environments?

There is an option for developers to get their database to run locally for the main application. This also means you run out-of-sync quickly with the other environments. On the other hand, the applets will still connect to backend services on dev, using the dev database. This leads to data incompatibilities. Is it my local database that is wrong? Is it the dev environment that is wrong? Or is it the combination that doesn't work? Running all 40 services locally is not an option. After starting the 8th service my machine completely froze, incapable of handling the memory-hungry jvms.

The dev environment runs on 7 machines. These machines are not hosted in any of the big cloud providers, instead they are managed by a managed cloud provider. There is no web interface to see or manage your instances as with any big cloud provider. The only way to get another instance is by requesting it (with a very good reason), getting it approved and waiting a number of days. The dev machines are running an older version of Ubuntu and have not seen a security patch since the day they were created.

Let's say you made changes to code and feel confident that it is going to work. When your changes are merged you need to get them deployed to the dev environment. The way to do that is by going to their Jenkins instance which, as you may expect by now, has not been updated since the day it has been set up. Jenkins contains jobs for each service. These jobs are not parameterized. As a result, each job is copied for every release for every environment. A view is created to group the jobs per release for environment. In a small amount of time this has lead to a completely filled up hard disk, and the regular task of cleaning up jobs in Jenkins to make space.

We are not there yet, you want your main application to be deployed right? Unfortunately the main application is a bit more difficult when it comes to building, so there is no job for it in Jenkins. Instead, you need to ssh into the Jenkins machine itself. You need to checkout the right branch for your repository and run the right script for building the docker containers and pushing them. Make sure to use the right build script as there is a different one for each environment. The only difference between these scripts is (or actually was supposed to be) the docker image tag. Please be careful with these scripts as they are not part of any code repository. The building does not start with a clean workspace. Also, the docker images, which include the code and keys to external parties, are pushed to the public docker hub. 

Alright, you got the images for your main application build. Now to get them deployed, ssh into the first machine of the dev environment, checkout the right branch of the repository with deployment scripts and run the one for dev. Now all you can do is hope.

The developer experience is terrible. You never know where to find code, there is no documentation, no tests, you are never confident your change won't break anything, and you have to work with unreliable environments. Your work grinds to a halt often due to a broken dev environment. Testing, building and deploying were manual steps that would often fail in mysterious ways.

The dev environment, and all other environments for that matter, did not have any aggregated logging, monitoring or alerting. The logs for dev could be accessed by checking the docker logs of the containers. Logs are not traceable for requests or for users. Quite often services would be down for extended periods. Issues on production would often be reported by tenants or their users. Issues are debugged by trying to reproduce them on dev or locally, many times to no avail.

## The Agile Process

I started this story with a number of questions. One of them is: "Are they doing agile development?" Well, the short answer is no, but they wanted to. They work in sprints of 4 weeks. That's a bit long to my taste. However, that is not the worst part of this. Each sprint consists of 2 weeks of development, 1 week of testing on QA and 1 week of testing on staging after which the deployment to production happens. You might be wondering, what are the developers are doing during the other two weeks? They are fixing bugs that have been introduced by their changes that have been tested, they are fixing bugs that have been reported by their customers and quite often they continue developing on the items that did not manage to be finished in the 2 weeks of development.

During the last 2 weeks of the sprint there are what they call grooming sessions. They take about an hour every day. In a grooming session the various product owners walk through the stories that are lined up for the coming sprint. The stories include requests from tenants, internal requests for more functionality and some reported bugs. Stories vary in size from a small enhancement to work that will take more than a sprint. Requests from tenants can be bugs or enhancements, quite often they are enhancements disguised as bugs. In case they are actual bugs they are often not more than a small email, possibly with a screenshot. In some cases the environment and login credentials are mentioned, but these credentials would not be tested, and the bug would not be reproduced or confirmed before the grooming session. This means that developers spend a substantial amount of time getting the information required to reproduce an issue. 

The grooming sessions are attended by product owners, developers, testers and the architect. The sessions consist mostly of walking through the newly requested functionality, and most of the questions are focused on the functionality itself. The technical aspects of stories are often not discussed at all or are deferred to other meetings.

Planning of what fits in the sprint is a difficult task in general. However, in this case the estimation is done by the product owners before the grooming sessions. They grade the complexity of a story as "easy", "medium" or "hard". How do you know if the groomed stories fit in the sprint? You don't. Nobody looks at the capacity, nobody tracks the velocity. In the end this means that every sprint will be filled up with items that the product owners would like to see in that sprint, not matter if it is realistic or not. This results in every sprint not being finished on time and big stories being completely transferred to the next sprint.

If you look at the types of stories I mentioned you might see that a critical one is missing: technical debt. Who is going to automate deployment? Who makes time for setting up a testing framework? Who is going to upgrade dependencies? No time is planned for any of the activities that make sure a developer is working in a workable environment. Because of this, dependencies of all services are stuck on where they were when they got included, the MongoDB database is stuck on it's initial version, the operating systems were living in the past. If you don't plan time for keeping up with the essentials, then how do you make time for improving the situation?

## The Way Forward

Software development is hard. Paying technical debt of several years is even harder. Where do you start?

Start by identifying the major problems. Keep a log where you record where you are spending your time. In this case time is wasted on a manual build process in a non-clean workspace, manual deployment to snowflake environments, and manual testing with non-trackable configurations. A lot of time is spent during the sprint on gathering requirements and reproducing issues.

We start by changing the grooming process to a refinement process in which the technical implications of any change are discussed and recorded. Create a Definition of Ready which is used to refine every story to a point at which a developer can pick it up and immediately start developing. Ensure all information for bugs is readily available in the story and let Product Owners verify them before they are talked about in a refinement session. Refine throughout the sprint to identify missing requirements, don't wait until the last moment. Prioritise the major problems and create stories for them. Reserve a certain amount of time each sprint for technical debt stories.

Look back at previous sprints and identify a number of reference stories. Take a story that took more than a sprint, a story that is finished in half a day and some stories in between. Use these stories to start estimating the complexity of the stories you are refining. Double the estimations to include time for creating tests, documentation and code reviews. Keep track of what is finished during a sprint to be able to plan which stories are going to fit in the next sprint. Initially it will feel like you are slowing down, but in fact you are paying your debt and making your predictions more reliable. If you can, to a certain degree, promise that your stories will be finished on time and create fewer problems, you are improving the situation.

Demo your achievements at the end of every week or to weeks. Use this as a moment of celebration for developers. Do a retrospective every week or to weeks to evaluate the process, to identify things can be improved and how they can be improved.

## Conclusion

I wish I could tell you that all problems have been resolved, but I'm not a magician. We set up a process which incorporated evaluation of the situation, identified problems and created time to improve the situation. The road ahead is a bumpy ride which will become smoother soon.